from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from selenium.webdriver.common.action_chains import ActionChains
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os
import time
import re

# Configuraci√≥n
OUTPUT_DIR = os.path.join("..", "data", "csvs-sucios")
os.makedirs(OUTPUT_DIR, exist_ok=True)
OUTPUT_PATH = os.path.join(OUTPUT_DIR, "voz_galicia_sucio.csv")
DATE_THRESHOLD = datetime(2025, 5, 15)

def setup_driver():
    """Configura Selenium con JavaScript habilitado para comentarios din√°micos"""
    chrome_options = Options()
    
    # Configuraci√≥n b√°sica - HABILITANDO JavaScript
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--no-sandbox") 
    chrome_options.add_argument("--disable-dev-shm-usage")
    
    # Headers realistas
    user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    chrome_options.add_argument(f"--user-agent={user_agent}")
    
    # Anti-detecci√≥n pero permitiendo JavaScript
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    try:
        driver = webdriver.Chrome(options=chrome_options)
        print("‚úÖ ChromeDriver iniciado con JavaScript habilitado")
    except Exception as e:
        print(f"‚ùå Error iniciando ChromeDriver: {e}")
        raise
    
    # Scripts anti-detecci√≥n
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

def wait_for_comments_to_load(driver, max_wait=30):
    """Espera agresivamente a que los comentarios se carguen"""
    print("üîÑ Esperando carga din√°mica de comentarios...")
    
    # Lista de selectores posibles para comentarios
    comment_selectors = [
        'dd.comment',
        'dd[class*="comment"]',
        '.comment-content',
        '[class*="comment"]',
        'div[id*="comment"]',
        '.comentario',
        '[class*="comentario"]'
    ]
    
    # Intentar multiple veces con diferentes estrategias
    for attempt in range(6):
        print(f"   üîÑ Intento {attempt + 1}/6...")
        
        # Hacer scroll hasta abajo
        for i in range(10):
            driver.execute_script(f"window.scrollBy(0, {i * 200});")
            time.sleep(0.3)

        
        # Buscar botones que puedan cargar comentarios
        try:
            load_buttons = driver.find_elements(By.CSS_SELECTOR, 
                "button[class*='comment'], button[class*='cargar'], button[class*='load'], .load-more, .show-comments")
            
            for button in load_buttons:
                try:
                    if button.is_displayed() and button.is_enabled():
                        print(f"   üñ±Ô∏è  Haciendo clic en bot√≥n: {button.get_attribute('class')}")
                        ActionChains(driver).move_to_element(button).click().perform()
                        time.sleep(2)
                except:
                    pass
        except:
            pass
        
        # Verificar si aparecieron comentarios
        for selector in comment_selectors:
            try:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    print(f"   ‚úÖ Encontrados {len(elements)} elementos con selector: {selector}")
                    time.sleep(2)  # Dar tiempo a que carguen completamente
                    return selector
            except:
                continue
        
        # Scroll intermedio
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
        time.sleep(2)
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)
    
    print("   ‚ö†Ô∏è  No se pudieron cargar comentarios din√°micamente")
    return None

def extract_comments_aggressive(driver, max_comments=15):
    """Extracci√≥n agresiva probando m√∫ltiples estrategias"""
    comments = []
    
    # Obtener HTML completo
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    time.sleep(2)  # Asegura que el DOM est√© completamente listo
    
    # Estrategia 1: Selectores espec√≠ficos de La Voz
    selectors_to_try = [
        'dl.comments > dd'
    ]
 
    comment_elements = []
    working_selector = None
    
    for selector in selectors_to_try:
        elements = soup.select(selector)
        if elements:
            print(f"‚úÖ Selector funcionando: {selector} ({len(elements)} elementos)")
            comment_elements = elements[:max_comments]
            working_selector = selector
            break
    
    if not comment_elements:
        print("‚ùå Ning√∫n selector espec√≠fico funcion√≥")
        # Estrategia 2: Buscar por contenido de texto
        print("üîç Buscando comentarios por contenido...")
        
        # Buscar elementos que contengan texto t√≠pico de comentarios
        all_elements = soup.find_all(['div', 'dd', 'article', 'section'])
        
        for element in all_elements:
            text = element.get_text().strip()
            
            # Patrones que indican comentarios
            comment_patterns = [
                r'hace \d+ d√≠as?',
                r'Responder?',
                r'Me gusta',
                r'No me gusta',
                r'desde [A-Z]+',
                r'[a-zA-Z]+[\.\-]\d+'  # Patrones de usuario como jan.-265425840
            ]
            
            pattern_matches = sum(1 for pattern in comment_patterns if re.search(pattern, text, re.IGNORECASE))
            
            # Si tiene al menos 2 patrones de comentario y texto sustancial
            if pattern_matches >= 2 and 20 < len(text) < 1000:
                comment_elements.append(element)
                if len(comment_elements) >= max_comments:
                    break
        
        print(f"üîç Encontrados {len(comment_elements)} elementos por contenido")
    
    if not comment_elements:
        print("‚ùå No se encontraron comentarios con ninguna estrategia")
        return []
    
    # Procesar elementos encontrados
    for i, element in enumerate(comment_elements, 1):
        try:
            print(f"üîé Procesando elemento {i}...")

            # Obtener texto inicial completo (con botones incluidos)
            full_text = element.get_text().strip()

            # Extraer likes/dislikes antes de eliminar botones
            likes = 0
            dislikes = 0

            like_match = re.search(r'Me gusta.*?(\d+)', full_text)
            if like_match:
                likes = int(like_match.group(1))

            dislike_match = re.search(r'No me gusta.*?(\d+)', full_text)
            if dislike_match:
                dislikes = int(dislike_match.group(1))

            # Ahora s√≠: eliminar botones del DOM (para limpiar el texto del comentario)
            for button in element.find_all("button"):
                button.decompose()

            # Volver a extraer texto limpio sin botones
            cleaned_text = element.get_text().strip()

            # Detectar si es una respuesta ANTES de limpiar el texto
            is_response = False
            response_to = ""
            response_match = re.search(r'(?:Responde\s+)?a\s+([a-zA-Z]+[\.\-_]*\d+)', cleaned_text)
            if response_match:
                is_response = True
                response_to = response_match.group(1)

            # Extraer autor
            author = "An√≥nimo"

            # Extraer ubicaci√≥n (ej: MUROS, FENE)
            location = ""
            location_match = re.search(r'desde\s+([A-Z√Å√â√ç√ì√ö√ë]{3,})', cleaned_text)
            if location_match:
                location = location_match.group(1)
            
            user_match = re.search(r'([a-zA-Z]+[\.\-]?\d+)', cleaned_text)

            if user_match:
                author = user_match.group(1)

            desde_match = re.search(r'desde ([A-Z][A-Z]+)', cleaned_text)
            if desde_match:
                author = f"{author} desde {desde_match.group(1)}"

            strong_elements = element.find_all('strong')
            for strong in strong_elements:
                strong_text = strong.get_text().strip()
                if len(strong_text) < 30 and strong_text not in ['Me gusta', 'No me gusta', 'Responder']:
                    if not re.search(r'hace \d+ d√≠as?', strong_text):
                        author = strong_text
                        break

            print(f"   üë§ Autor detectado: {author}")

            # Extraer fecha
            date_text = ""
            date_match = re.search(r'hace \d+ d√≠as?\.?', cleaned_text)
            if date_match:
                date_text = date_match.group(0)

            print(f"   üìÖ Fecha detectada: {date_text}")

            # Texto del comentario limpio
            comment_text = cleaned_text

            cleanup_patterns = [
                r'hace \d+ d√≠as?\.?',
                r'Me gusta.*?\d+',
                r'No me gusta.*?\d+',
                r'\bResponder\b',
                r'\bdesde\s+[A-Z√Å√â√ç√ì√ö√ë]{3,}(?:\s+y\s+\w+)?[\.,]?',      # desde FENE, etc.
            ]

            # Limpiar patrones b√°sicos
            for pattern in cleanup_patterns:
                comment_text = re.sub(pattern, '', comment_text, flags=re.IGNORECASE)

            # Limpiar usuario al principio (sin "Responde a")
            if not is_response:
                comment_text = re.sub(r'^[a-zA-Z]+[\.\-_]*\d+\s+', '', comment_text, flags=re.IGNORECASE)
            else:
                # Para respuestas: cambiar "a usuario" por "Responde a usuario"
                comment_text = re.sub(r'^a\s+([a-zA-Z]+[\.\-_]*\d+)\.?\s*', r'Responde a \1. ', comment_text, flags=re.IGNORECASE)
            
            comment_text = ' '.join(comment_text.split()).strip()

            print(f"   üí¨ Texto limpio: {comment_text[:100]}...")

            if comment_text and len(comment_text) > 10:
                if any(c["text"] == comment_text for c in comments):
                    print(f"   ‚ö†Ô∏è Comentario {i} duplicado (mismo texto), descartado")
                    continue

                comments.append({
                    "author": author,
                    "location": location,
                    "text": comment_text,
                    "date": date_text,
                    "likes": likes,
                    "dislikes": dislikes
                })
                print(f"   ‚úÖ Comentario {i} agregado")
            else:
                print(f"   ‚ö†Ô∏è  Comentario {i} descartado (texto insuficiente)")

        except Exception as e:
            print(f"‚ùå Error procesando elemento {i}: {e}")
            continue
    
    return comments

def extract_comments_selenium(url, max_comments=15):
    """Funci√≥n principal de extracci√≥n con m√∫ltiples estrategias"""
    driver = setup_driver()
    comments = []

    try:
        print(f"üåê Cargando URL: {url}")
        driver.get(url)
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.ID, "comments_feed"))
            )
            print("‚úÖ Comentarios din√°micos detectados")
        except TimeoutException:
            print("‚ö†Ô∏è Timeout esperando #comments_feed (puede que no haya comentarios visibles)")

        print(f"üìÑ P√°gina cargada: {driver.title}")
        
        # Esperar a que se carguen los comentarios din√°micamente
        working_selector = wait_for_comments_to_load(driver)
        
        # Extracci√≥n agresiva
        wait_for_comments_to_load(driver)
        comments = extract_comments_aggressive(driver)

        
        # Si no encuentra comentarios, guardar HTML para debug
        if not comments:
            debug_file = "debug_la_voz_comments.html"
            with open(debug_file, "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            print(f"üíæ HTML guardado en {debug_file} para debugging")
        
    except Exception as e:
        print(f"‚ùå Error general: {e}")
    finally:
        driver.quit()
    
    return comments

def scrape_and_save():
    """Funci√≥n principal para un solo art√≠culo"""
    url = "https://www.lavozdegalicia.es/noticia/sociedad/2025/05/15/playa-portocelo-leyenda-pueblo-hundido/0003_202505G15P28996.htm"
    
    print("üöÄ Iniciando scraping agresivo de La Voz de Galicia...")
    comments = extract_comments_selenium(url)
    
    if comments:
        print(f"üéâ ¬°√âxito! Encontrados {len(comments)} comentarios")
    else:
        print("‚ö†Ô∏è  No se encontraron comentarios")
    
    # Crear estructura de datos
    article_data = {
        "source": "La Voz de Galicia",
        "title": "Playa de Portocelo: La leyenda del pueblo hundido",
        "link": url,
        "date": datetime.now().isoformat(),
        "n_comments": len(comments)
    }

    # Crear campos agrupados por comentario
    for i in range(15):
        article_data.update({
            f"comment_{i+1}_author": "",
            f"comment_{i+1}_location": "",
            f"comment_{i+1}_date": "",
            f"comment_{i+1}_text": "",
            f"comment_{i+1}_likes": 0,
            f"comment_{i+1}_dislikes": 0
        })

    # Llenar con comentarios encontrados
    for i, comment in enumerate(comments[:15]):
        article_data[f"comment_{i+1}_author"] = comment["author"]
        article_data[f"comment_{i+1}_location"] = comment["location"]
        article_data[f"comment_{i+1}_date"] = comment["date"]
        article_data[f"comment_{i+1}_text"] = comment["text"]
        article_data[f"comment_{i+1}_likes"] = comment["likes"]
        article_data[f"comment_{i+1}_dislikes"] = comment["dislikes"]
    
    # Guardar CSV
    df = pd.DataFrame([article_data])
    df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')
    
    print(f"‚úÖ CSV guardado en: {os.path.abspath(OUTPUT_PATH)}")
    print(f"üìä Total comentarios: {len(comments)}")
    
    if comments:
        print("\nüìã Comentarios extra√≠dos:")
        for i, comment in enumerate(comments[:5], 1):
            print(f"   {i}. [{comment['author']}] {comment['text'][:80]}...")
            if comment['date']:
                print(f"      üìÖ {comment['date']} | üëç {comment['likes']} | üëé {comment['dislikes']}")

def scrape_multiple_articles(base_url="https://www.lavozdegalicia.es/pontevedra/marin/",
                            date_threshold=datetime(2025, 5, 15),
                            max_articles=1000,
                            max_old_articles=3): 
    """Funci√≥n para scraper m√∫ltiples art√≠culos"""
    driver = setup_driver()
    article_count = 0
    old_articles_seen = 0
    page = 1
    all_rows = []

    print(f"üéØ Scrapeando art√≠culos posteriores a: {date_threshold.date()}")
    print(f"üìä M√°ximo de art√≠culos antiguos consecutivos antes de parar: {max_old_articles}")

    try:
        while article_count < max_articles and old_articles_seen < max_old_articles:
            page_url = base_url if page == 1 else f"{base_url}{page}"
            print(f"\nüåê Cargando p√°gina {page}: {page_url}")
            driver.get(page_url)
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            soup = BeautifulSoup(driver.page_source, 'html.parser')

            article_links = []
            for a in soup.select('a[href*="/noticia/"]'):
                href = a.get('href')
                if href and href.startswith("/noticia/"):
                    full_url = f"https://www.lavozdegalicia.es{href.split('?')[0]}"
                    if full_url not in article_links:
                        article_links.append(full_url)

            print(f"üîó Encontrados {len(article_links)} enlaces √∫nicos")
            
            if not article_links:
                print("‚ùå No se encontraron m√°s art√≠culos, terminando...")
                break

            for article_url in article_links:
                if article_count >= max_articles:
                    print(f"‚úÖ Alcanzado l√≠mite m√°ximo de art√≠culos ({max_articles})")
                    break
                    
                if old_articles_seen >= max_old_articles:
                    print(f"üõë Encontrados {max_old_articles} art√≠culos antiguos consecutivos, parando...")
                    break

                print(f"\nüìù Procesando art√≠culo: {article_url}")
                try:
                    driver.get(article_url)
                    WebDriverWait(driver, 10).until(
                        EC.presence_of_element_located((By.TAG_NAME, "body"))
                    )

                    soup_article = BeautifulSoup(driver.page_source, 'html.parser')

                    # Extraer fecha del art√≠culo con m√∫ltiples estrategias
                    article_date = None
                    
                    # Estrategia 1: Tag time con datetime
                    time_tag = soup_article.find('time')
                    if time_tag and time_tag.has_attr('datetime'):
                        try:
                            date_str = time_tag['datetime']
                            article_date = datetime.fromisoformat(date_str[:10])
                            print(f"   üìÖ Fecha extra√≠da de time tag: {article_date.date()}")
                        except:
                            pass
                    
                    # Estrategia 2: Buscar en la URL (formato /YYYY/MM/DD/)
                    if not article_date:
                        url_date_match = re.search(r'/(\d{4})/(\d{2})/(\d{2})/', article_url)
                        if url_date_match:
                            try:
                                year, month, day = url_date_match.groups()
                                article_date = datetime(int(year), int(month), int(day))
                                print(f"   üìÖ Fecha extra√≠da de URL: {article_date.date()}")
                            except:
                                pass
                    
                    # Estrategia 3: Buscar patrones de fecha en el HTML
                    if not article_date:
                        date_patterns = [
                            r'(\d{4}-\d{2}-\d{2})',
                            r'(\d{2}/\d{2}/\d{4})',
                            r'(\d{1,2}\s+de\s+\w+\s+de\s+\d{4})'
                        ]
                        page_text = soup_article.get_text()
                        for pattern in date_patterns:
                            match = re.search(pattern, page_text)
                            if match:
                                try:
                                    date_str = match.group(1)
                                    if '-' in date_str:
                                        article_date = datetime.fromisoformat(date_str)
                                    # Agregar m√°s parsers si es necesario
                                    print(f"   üìÖ Fecha extra√≠da del texto: {article_date.date()}")
                                    break
                                except:
                                    continue
                    
                    # Si no se pudo extraer fecha, usar fecha muy reciente para no filtrar
                    if not article_date:
                        article_date = datetime.now()
                        print(f"   ‚ö†Ô∏è No se pudo extraer fecha, usando fecha actual: {article_date.date()}")
                    
                    # Verificar si el art√≠culo es anterior a la fecha umbral
                    if article_date.date() < date_threshold.date():
                        old_articles_seen += 1
                        print(f"   ‚è≥ Art√≠culo anterior a fecha umbral ({article_date.date()} < {date_threshold.date()})")
                        print(f"   üìä Art√≠culos antiguos consecutivos: {old_articles_seen}/{max_old_articles}")
                        continue
                    else:
                        # Reset contador si encontramos un art√≠culo reciente
                        old_articles_seen = 0

                    # Cargar comentarios din√°micos y luego extraerlos
                    wait_for_comments_to_load(driver)
                    comments = extract_comments_aggressive(driver)

                    row = {
                        "source": "La Voz de Galicia",
                        "title": soup_article.title.string if soup_article.title else "",
                        "link": article_url,
                        "date": article_date.isoformat(),
                        "n_comments": len(comments)
                    }

                    for i in range(15):
                        row.update({
                            f"comment_{i+1}_author": "",
                            f"comment_{i+1}_location": "",
                            f"comment_{i+1}_date": "",
                            f"comment_{i+1}_text": "",
                            f"comment_{i+1}_likes": 0,
                            f"comment_{i+1}_dislikes": 0
                        })

                    for i, comment in enumerate(comments[:15]):
                        row[f"comment_{i+1}_author"] = comment["author"]
                        row[f"comment_{i+1}_location"] = comment["location"]
                        row[f"comment_{i+1}_date"] = comment["date"]
                        row[f"comment_{i+1}_text"] = comment["text"]
                        row[f"comment_{i+1}_likes"] = comment["likes"]
                        row[f"comment_{i+1}_dislikes"] = comment["dislikes"]

                    all_rows.append(row)
                    article_count += 1

                except Exception as e:
                    print(f"‚ùå Error procesando art√≠culo: {e}")
                    continue

            page += 1

    finally:
        driver.quit()

    # Guardar resultados
    if all_rows:
        df = pd.DataFrame(all_rows)
        df.to_csv(OUTPUT_PATH, index=False, encoding='utf-8')
        print(f"\n‚úÖ Guardado CSV con {len(df)} art√≠culos en {OUTPUT_PATH}")
    else:
        print(f"\n‚ö†Ô∏è No se encontraron art√≠culos v√°lidos para guardar")
    
    # Resumen final
    print(f"\nüìä RESUMEN DEL SCRAPING:")
    print(f"   üì∞ Art√≠culos procesados: {article_count}")
    print(f"   üìÖ Fecha umbral: {date_threshold.date()}")
    print(f"   ‚è≥ Art√≠culos antiguos consecutivos encontrados: {old_articles_seen}")
    
    if old_articles_seen >= max_old_articles:
        print(f"   üõë Par√≥ por encontrar {max_old_articles} art√≠culos antiguos consecutivos")
    elif article_count >= max_articles:
        print(f"   ‚úÖ Par√≥ por alcanzar el l√≠mite m√°ximo de art√≠culos ({max_articles})")
    else:
        print(f"   üèÅ Par√≥ por no encontrar m√°s art√≠culos")

# PUNTO DE ENTRADA PRINCIPAL
if __name__ == "__main__":
    # Configuraci√≥n de fecha umbral
    fecha_umbral = datetime(2025, 5, 15)  # Cambiar aqu√≠ la fecha l√≠mite
    
    print(f"üéØ Configuraci√≥n: Solo art√≠culos posteriores a {fecha_umbral.date()}")
    
    # Descomenta la funci√≥n que quieras ejecutar:
    
    # Para scraper un solo art√≠culo (NO usa DATE_THRESHOLD):
    # scrape_and_save()
    
    # Para scraper m√∫ltiples art√≠culos CON filtro de fecha:
    scrape_multiple_articles(
        date_threshold=fecha_umbral,
        max_articles=100,  # Ajustar seg√∫n necesidades
        max_old_articles=5  # Parar despu√©s de 5 art√≠culos antiguos consecutivos
    )