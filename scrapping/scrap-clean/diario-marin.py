import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import re
from dateutil.parser import parse as date_parse
import time
import os

HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
DATE_THRESHOLD = datetime(2023, 5, 28)

def scrape_diario_marin_all():
    base_url = "https://diariomarin.com"
    page = 1
    data = []
    MAX_PAGES = 1000  # L√≠mite de seguridad para evitar bucles infinitos
    seen_links = set()  # Para evitar duplicados
    consecutive_empty_pages = 0  # Contador de p√°ginas vac√≠as consecutivas
    old_article_count = 0  # Contador de art√≠culos antiguos consecutivos
    MAX_OLD_ARTICLES = 3  # N√∫mero de art√≠culos antiguos consecutivos para detener

    while page <= MAX_PAGES:
        page_url = f"{base_url}/page/{page}/" if page > 1 else base_url
        print(f"\nüåÄ Cargando P√°gina {page}: {page_url}")
        
        try:
            response = requests.get(page_url, headers=HEADERS, timeout=15)
            if response.status_code != 200:
                print(f"‚ùå Fin de p√°ginas o error HTTP: {response.status_code}")
                break
        except Exception as e:
            print(f"‚ùå Error al cargar p√°gina: {e}")
            break

        soup = BeautifulSoup(response.text, "html.parser")
        
        # MEJOR IDENTIFICACI√ìN DE ART√çCULOS - ENFOQUE M√ÅS AMPLIO
        articles = []
        
        # Probar con varios selectores para mayor robustez
        # 1. M√©todo directo para art√≠culos
        articles = soup.find_all("article")
        
        # 2. Buscar en contenedores conocidos si m√©todo 1 falla
        if not articles:
            possible_containers = [
                soup.find('div', class_='td-ss-main-content'),
                soup.find('div', class_=['tdb_loop', 'td_block_wrap tdb_loop']),
                soup.find('div', id='tdi_52'),
                # A√±adir m√°s contenedores potenciales aqu√≠
                soup.find('div', class_=lambda c: c and ('content' in c.lower() if c else False)),
                soup.find('div', class_=lambda c: c and ('loop' in c.lower() if c else False)),
                soup.find('main'),  # A veces los art√≠culos est√°n en el <main>
            ]
            
            for container in possible_containers:
                if container:
                    container_articles = container.find_all('article')
                    if container_articles:
                        articles = container_articles
                        break
                    
                    # Si no encuentra <article> directamente, buscar divs que contengan t√≠tulos
                    potential_article_divs = container.find_all('div', class_=lambda c: c and ('item' in c.lower() if c else False))
                    if potential_article_divs:
                        articles = potential_article_divs
                        break
        
        if not articles:
            print(f"‚ö†Ô∏è No se encontraron art√≠culos en la p√°gina {page}.")
            consecutive_empty_pages += 1
            if consecutive_empty_pages >= 2:  # Si 2 p√°ginas consecutivas no tienen art√≠culos, terminar
                print("‚ùå M√∫ltiples p√°ginas vac√≠as consecutivas. Finalizando.")
                break
            page += 1
            continue
        else:
            consecutive_empty_pages = 0  # Reiniciar contador si encontramos art√≠culos
        
        print(f"üìÑ Art√≠culos encontrados: {len(articles)}")
        valid_articles_this_page = 0
        new_articles_this_page = 0
           # Proceso completo de art√≠culos
        for article in articles:
            try:
                # EXTRACCI√ìN M√ÅS ROBUSTA DEL T√çTULO Y ENLACE
                # Intenta varios m√©todos para encontrar t√≠tulo y enlace
                title_tag = None
                link = None
                
                # M√©todo 1: Buscar en encabezados con clases espec√≠ficas
                for header_tag in ['h2', 'h3', 'h4']:
                    if title_tag:
                        break
                    potential_tags = article.find_all(header_tag, class_=lambda c: c and 
                                                     any(cls in (c or '') for cls in ['entry-title', 'title', 'tdb-entry-title']))
                    if potential_tags:
                        title_tag = potential_tags[0]
                
                # M√©todo 2: Cualquier encabezado si no encontramos con clases espec√≠ficas
                if not title_tag:
                    for header_tag in ['h2', 'h3', 'h4', 'h1']:
                        potential_tags = article.find_all(header_tag)
                        if potential_tags:
                            title_tag = potential_tags[0]
                            break
                
                # M√©todo 3: Buscar cualquier etiqueta con la clase que contiene "title"
                if not title_tag:
                    title_elements = article.find_all(class_=lambda c: c and 'title' in c.lower() if c else False)
                    if title_elements:
                        title_tag = title_elements[0]
                
                # Extraer t√≠tulo y enlace
                if title_tag:
                    title = title_tag.get_text(strip=True)
                    
                    # Intentar obtener enlace del t√≠tulo
                    link_tag = title_tag.find('a')
                    if link_tag and 'href' in link_tag.attrs:
                        link = link_tag['href']
                    else:
                        # Buscar cualquier enlace en el art√≠culo
                        any_link = article.find('a')
                        if any_link and 'href' in any_link.attrs:
                            link = any_link['href']
                
                # Asegurar que tenemos t√≠tulo y enlace
                if not title_tag or not link:
                    print("‚ö†Ô∏è No se pudo extraer t√≠tulo o enlace. Saltando art√≠culo.")
                    continue
                
                # Normalizar URL (a√±adir dominio si es relativa)
                if link and not link.startswith(('http://', 'https://')):
                    link = base_url + ('' if link.startswith('/') else '/') + link
                
                # Verificar duplicados
                if link in seen_links:
                    print(f"üëÄ Art√≠culo duplicado: {title}. Saltando.")
                    continue
                
                seen_links.add(link)
                new_articles_this_page += 1

                # Cargar post completo
                try:
                    post_response = requests.get(link, headers=HEADERS, timeout=15)
                    post_page = BeautifulSoup(post_response.text, "html.parser")
                except Exception as e:
                    print(f"‚ö†Ô∏è Error al cargar art√≠culo '{title}': {str(e)[:100]}")
                    continue

                # EXTRACCI√ìN MEJORADA DE FECHA - M√öLTIPLES M√âTODOS
                date = None
                
                # M√©todo 1: Meta tags (m√°s confiable)
                meta_tags = [
                    post_page.find("meta", {"property": "article:published_time"}),
                    post_page.find("meta", {"name": "article:published_time"}),
                    post_page.find("meta", {"property": "og:published_time"}),
                    post_page.find("meta", {"name": "publication_date"})
                ]
                
                for tag in meta_tags:
                    if tag and tag.get("content"):
                        try:
                            date = date_parse(tag["content"]).replace(tzinfo=None)
                            break
                        except:
                            continue
                
                # M√©todo 2: Etiquetas time
                if not date:
                    time_tags = post_page.find_all("time")
                    for t in time_tags:
                        if "datetime" in t.attrs:
                            try:
                                date = date_parse(t["datetime"]).replace(tzinfo=None)
                                break
                            except:
                                continue
                
                # M√©todo 3: Buscar en clases que contengan "date"
                if not date:
                    date_elements = post_page.find_all(class_=lambda c: c and 'date' in c.lower() if c else False)
                    for element in date_elements:
                        try:
                            date_text = element.get_text(strip=True)
                            date = date_parse(date_text).replace(tzinfo=None)
                            break
                        except:
                            continue
                
                if not date:
                    print(f"‚ö†Ô∏è No se pudo obtener fecha para '{title}'. Se ignora.")
                    continue

                # COMPROBACI√ìN DE FECHA UMBRAL - MODIFICADO
                if date < DATE_THRESHOLD:
                    print(f"üö´ Art√≠culo con fecha {date.isoformat()} anterior al umbral {DATE_THRESHOLD.isoformat()}. Descartando.")
                    old_article_count += 1  # Incrementar contador de art√≠culos antiguos
                    
                    # Si encontramos suficientes art√≠culos antiguos consecutivos, terminamos
                    if old_article_count >= MAX_OLD_ARTICLES:
                        print(f"üèÅ Se encontraron {MAX_OLD_ARTICLES} art√≠culos consecutivos anteriores al umbral. Finalizando.")
                        break
                    
                    continue  # Saltamos este art√≠culo
                else:
                    # Resetear contador si encontramos un art√≠culo reciente
                    old_article_count = 0

                # ‚úÖ Si llegamos aqu√≠, el post es v√°lido
                valid_articles_this_page += 1
                print(f"‚úÖ [Diario Mar√≠n] {title} | {date.isoformat()} -> V√ÅLIDO")

                # Extracci√≥n mejorada de comentarios
                comment_texts = []
                processed_comments = set()  # Conjunto para evitar duplicados
                
                # M√©todo 1: Buscar por clase comment-list
                comment_blocks = post_page.find_all(["ol", "ul", "div"], class_=lambda c: c and 'comment' in c.lower() if c else False)
                
                for block in comment_blocks:
                    comments = block.find_all(["li", "div"], class_=lambda c: c and ('comment' in c.lower() if c else False) and ('form' not in c.lower() if c else True))
                    for comment in comments:
                        # Intentar extraer solo el texto del comentario, no metadatos
                        comment_body = comment.find(class_=lambda c: c and ('text' in c.lower() or 'body' in c.lower() or 'content' in c.lower() if c else False))
                        
                        if comment_body:
                            c = comment_body.get_text(separator=" ", strip=True)
                        else:
                            c = comment.get_text(separator=" ", strip=True)
                            
                        # Ignorar texto que no es un comentario real
                        if c and len(c) > 10:  # Filtrar textos muy cortos que probablemente sean metadata
                            # Ignorar textos espec√≠ficos que no son comentarios reales
                            if not any(texto in c for texto in ["Deja un comentario", "Cancelar respuesta", "Responder"]):
                                # Solo a√±adir si no es un duplicado
                                if c not in processed_comments:
                                    processed_comments.add(c)
                                    comment_texts.append(c)

                # Filtrar a√∫n m√°s para mantener solo comentarios en formato preferido
                # Reemplaza TODO el bloque de extracci√≥n de comentarios por esto
                final_comments = []
                comment_elements = post_page.select("ol.commentlist li.comment")

                for comment in comment_elements[:15]:
                    try:
                    # Autor (opcional, no se usar√° ahora)
                        author_tag = comment.select_one(".comment-author .fn")
                        author = author_tag.get_text(strip=True) if author_tag else "An√≥nimo"

                        # Fecha
                        date_tag = comment.select_one("footer .comment-metadata")
                        date_text = date_tag.get_text(strip=True) if date_tag else ""
                        date_text = re.sub(r"\s+", " ", date_text)

                        # Contenido
                        content_tag = comment.select_one(".comment-content")
                        content = content_tag.get_text(separator=" ", strip=True) if content_tag else ""
                        content = re.sub(r"\s+", " ", content)

                        # Validaci√≥n m√≠nima
                        if len(content) > 5:
                            final_comments.append({
                                "author": author,
                                "date": date_text,
                                "text": content
                            })
                    except Exception as e:
                        print(f"‚ö†Ô∏è Error al procesar comentario: {str(e)[:100]}")
                        continue

                # Calcular n√∫mero real de comentarios (excluyendo textos no deseados)
                # Estructura con comentarios separados hasta 15
                row = {
                "source": "Diario Mar√≠n",
                "title": title,
                "link": link,
                "date": date.isoformat(),
                "n_comments": len(final_comments)
                }

                for i in range(15):
                    row[f"comment_{i+1}_author"] = ""
                    row[f"comment_{i+1}_date"] = ""
                    row[f"comment_{i+1}_text"] = ""

                for i, comment in enumerate(final_comments[:15]):
                    row[f"comment_{i+1}_author"] = comment["author"]
                    row[f"comment_{i+1}_date"] = comment["date"]
                    row[f"comment_{i+1}_text"] = comment["text"]

                data.append(row)


            except Exception as e:
                print(f"‚ö†Ô∏è Error procesando art√≠culo: {str(e)[:100]}...")
                continue

        # VERIFICACI√ìN SI SE ALCANZ√ì EL L√çMITE DE ART√çCULOS ANTIGUOS
        if old_article_count >= MAX_OLD_ARTICLES:
            print(f"üèÅ Se alcanz√≥ el l√≠mite de art√≠culos antiguos consecutivos ({MAX_OLD_ARTICLES}). Finalizando.")
            break
                
        # MEJOR CONTROL DE PAGINACI√ìN
        print(f"üìä P√°gina {page}: {valid_articles_this_page} art√≠culos v√°lidos, {new_articles_this_page} nuevos")
        
        # Criterios de finalizaci√≥n mejorados
        if new_articles_this_page == 0:
            print(f"‚ö†Ô∏è No se encontraron nuevos art√≠culos en la p√°gina {page}. Verificando una p√°gina m√°s...")
            consecutive_empty_pages += 1
            if consecutive_empty_pages >= 2:
                print("‚ùå Fin de paginaci√≥n detectado (sin nuevos art√≠culos). Finalizando.")
                break
            
        # Restablecer el contador de art√≠culos antiguos entre p√°ginas
        old_article_count = 0  # Resetear entre p√°ginas para evitar falsos positivos
                
        # Pausa para no sobrecargar el servidor
        page += 1
        time.sleep(2)  # Pausa m√°s larga para evitar problemas

    print(f"‚úÖ Diario Mar√≠n: {len(data)} art√≠culos v√°lidos recopilados de {page-1} p√°ginas")
    print(f"üìÖ Rango de fechas: {data[0]['date'] if data else 'N/A'} a {data[-1]['date'] if data else 'N/A'}")
    return data

# Ejecutar - Corrigiendo el error: debe ser scrape_diario_marin_all()
data = scrape_diario_marin_all()

# Ruta a nivel superior (subir un nivel desde ialimpia)
output_dir = os.path.join("..", "..", "data", "raw", "clean-csvs")
os.makedirs(output_dir, exist_ok=True)  # Crea la estructura si no existe

# Ruta completa del archivo CSV
csv_path = os.path.join(output_dir, "diario-marin-limpio.csv")

# Guardar CSV en la carpeta correcta
df = pd.DataFrame(data)
df.to_csv(csv_path, index=False)

# Mostrar resumen
print(f"\nüì¶ CSV generado en '{csv_path}' con {len(df)} art√≠culos.")
print(df[["source", "title", "date", "n_comments"]])