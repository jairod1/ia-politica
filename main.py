#!/usr/bin/env python3
"""
MAIN PIPELINE - HorizontAI
===========================

Pipeline completo que ejecuta todos los scrapers y procesamiento de datos
para generar los CSVs finales utilizados por la aplicaciÃ³n Streamlit.

PIPELINE:
1. SCRAPERS â†’ Datos raw individuales
2. COMBINACIÃ“N â†’ Datos unificados  
3. FILTRADO â†’ Datos procesados para Streamlit
4. MÃ‰TRICAS â†’ Datos de visualizaciones procesados
"""

import os
import sys
import time
import subprocess
import logging
from datetime import datetime
from pathlib import Path

# ========================
# CONFIGURACIÃ“N PRINCIPAL
# ========================

# Configurar logging detallado
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('pipeline_execution.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Paths del proyecto
PROJECT_ROOT = Path(__file__).parent
SCRAPING_DIR = PROJECT_ROOT / "src" / "scrapping" / "scrap-clean"
SRC_DIR = PROJECT_ROOT / "src"
DATA_RAW_DIR = PROJECT_ROOT / "data" / "raw"
DATA_PROCESSED_DIR = PROJECT_ROOT / "data" / "processed"

# ConfiguraciÃ³n de scrapers
SCRAPERS_CONFIG = [
    {
        "name": "Carriola de MarÃ­n",
        "script": "carriola-marin.py",
        "output_dir": "clean-metrics",
        "output_file": "carriola-marin.csv",
        "delay_after": 30  # Segundos de pausa despuÃ©s del scraper
    },
    {
        "name": "Cousas de Carragal", 
        "script": "cousas-carragal.py",
        "output_dir": "clean-csvs",
        "output_file": "cousas-carragal-limpio.csv",
        "delay_after": 30
    },
    {
        "name": "Diario MarÃ­n",
        "script": "diario-marin.py", 
        "output_dir": "clean-csvs",
        "output_file": "diario-marin-limpio.csv",
        "delay_after": 30
    },
    {
        "name": "Diario Pontevedra",
        "script": "diario-pontevedra.py",
        "output_dir": "clean-csvs", 
        "output_file": "diario-pontevedra-limpio.csv",
        "delay_after": 60  # MÃ¡s tiempo por el anti-detecciÃ³n
    },
    {
        "name": "PSOE MarÃ­n",
        "script": "psdeg-marin.py",
        "output_dir": "clean-csvs",
        "output_file": "psdeg-marin-limpio.csv", 
        "delay_after": 30
    },
    {
        "name": "Voz de Galicia",
        "script": "voz-galicia.py",
        "output_dir": "clean-csvs",
        "output_file": "voz_galicia-limpio.csv",
        "delay_after": 45  # Selenium requiere mÃ¡s tiempo
    }
]

# ConfiguraciÃ³n de procesadores
PROCESSORS_CONFIG = [
    {
        "name": "Combinador de Datos",
        "script": "data-combiner.py",
        "directory": "comments",
        "description": "Combina todos los CSVs individuales en combined_data.csv"
    },
    {
        "name": "Filtro Avanzado",
        "script": "filter-advanced.py", 
        "directory": "comments",
        "description": "Genera filtered_data.csv y filtros especÃ­ficos"
    },
    {
        "name": "Procesador MÃ©tricas Avanzadas",
        "script": "filter-advanced-m.py",
        "directory": "metrics", 
        "description": "Procesa mÃ©tricas polÃ­ticas (politicos_totales.csv)"
    },
    {
        "name": "Procesador MÃ©tricas BÃ¡sicas", 
        "script": "filter-basic-m.py",
        "directory": "metrics",
        "description": "Procesa mÃ©tricas de visualizaciones (visualizaciones_totales.csv)"
    }
]

# ========================
# FUNCIONES AUXILIARES
# ========================

def print_banner():
    """Banner de inicio"""
    banner = """
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                     ğŸš€ HORIZONTAI PIPELINE ğŸš€               â•‘
    â•‘                                                              â•‘
    â•‘  Pipeline completo de scraping y procesamiento de datos      â•‘
    â•‘  para anÃ¡lisis polÃ­tico de medios locales                    â•‘
    â•‘                                                              â•‘
    â•‘   6 Scrapers â†’ CombinaciÃ³n â†’ Filtrado â†’ CSVs finales         â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(banner)
    logger.info("Iniciando pipeline completo HorizontAI")

def create_directories():
    """Crea directorios necesarios si no existen"""
    directories = [
        DATA_RAW_DIR / "clean-csvs",
        DATA_RAW_DIR / "clean-metrics", 
        DATA_PROCESSED_DIR / "combined-data",
        DATA_PROCESSED_DIR / "filtered-data",
        DATA_PROCESSED_DIR / "metrics-data",
        DATA_PROCESSED_DIR / "metrics-advanced"
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
        logger.info(f"âœ… Directorio verificado: {directory}")

def check_file_exists(file_path, description="archivo"):
    """Verifica si un archivo existe y reporta su tamaÃ±o"""
    if file_path.exists():
        size_mb = file_path.stat().st_size / (1024 * 1024)
        logger.info(f"âœ… {description} generado: {file_path.name} ({size_mb:.2f} MB)")
        return True
    else:
        logger.error(f"âŒ {description} NO encontrado: {file_path}")
        return False

def run_script(script_path, description, working_dir=None):
    """Ejecuta un script Python y maneja errores"""
    logger.info(f"ğŸ”„ Ejecutando: {description}")
    logger.info(f"   Script: {script_path}")
    
    if working_dir:
        logger.info(f"   Directorio: {working_dir}")
    
    try:
        # Cambiar al directorio de trabajo si se especifica
        original_cwd = os.getcwd()
        if working_dir:
            os.chdir(working_dir)
        
        # Ejecutar script
        result = subprocess.run(
            [sys.executable, str(script_path)],
            capture_output=True,
            text=True,
            timeout=3600  # 1 hora mÃ¡ximo por script
        )
        
        # Restaurar directorio original
        os.chdir(original_cwd)
        
        if result.returncode == 0:
            logger.info(f"âœ… {description} completado exitosamente")
            if result.stdout:
                logger.info(f"   Output: {result.stdout[-200:]}")  # Ãšltimas 200 chars
            return True
        else:
            logger.error(f"âŒ {description} fallÃ³ (cÃ³digo: {result.returncode})")
            if result.stderr:
                logger.error(f"   Error: {result.stderr}")
            if result.stdout:
                logger.error(f"   Output: {result.stdout}")
            return False
            
    except subprocess.TimeoutExpired:
        logger.error(f"â° {description} excediÃ³ tiempo lÃ­mite (1 hora)")
        return False
    except Exception as e:
        logger.error(f"ğŸ’¥ Error ejecutando {description}: {e}")
        return False
    finally:
        # Asegurar que volvemos al directorio original
        os.chdir(original_cwd)

def countdown_delay(seconds, message):
    """Cuenta regresiva visual"""
    logger.info(f"â³ {message}")
    for i in range(seconds, 0, -1):
        print(f"   â±ï¸  Esperando {i} segundos...", end='\r', flush=True)
        time.sleep(1)
    print("   âœ… Pausa completada" + " " * 20)  # Limpiar lÃ­nea

# ========================
# FUNCIONES PRINCIPALES  
# ========================

def step_1_run_scrapers():
    """PASO 1: Ejecutar todos los scrapers"""
    logger.info("=" * 60)
    logger.info("ğŸ“¡ PASO 1: EJECUTANDO SCRAPERS")
    logger.info("=" * 60)
    
    successful_scrapers = 0
    failed_scrapers = 0
    
    for i, scraper in enumerate(SCRAPERS_CONFIG, 1):
        logger.info(f"\nğŸ¯ SCRAPER {i}/{len(SCRAPERS_CONFIG)}: {scraper['name']}")
        logger.info("-" * 50)
        
        # Path del script
        script_path = SCRAPING_DIR / scraper['script']
        
        # Verificar que el script existe
        if not script_path.exists():
            logger.error(f"âŒ Script no encontrado: {script_path}")
            failed_scrapers += 1
            continue
        
        # Ejecutar scraper
        success = run_script(
            script_path, 
            f"Scraper {scraper['name']}", 
            working_dir=SCRAPING_DIR
        )
        
        if success:
            # Verificar que se generÃ³ el archivo
            output_path = DATA_RAW_DIR / scraper['output_dir'] / scraper['output_file']
            if check_file_exists(output_path, f"CSV de {scraper['name']}"):
                successful_scrapers += 1
            else:
                logger.warning(f"âš ï¸ {scraper['name']} ejecutÃ³ pero no generÃ³ archivo esperado")
                failed_scrapers += 1
        else:
            failed_scrapers += 1
        
        # Pausa entre scrapers (excepto el Ãºltimo)
        if i < len(SCRAPERS_CONFIG):
            countdown_delay(
                scraper['delay_after'], 
                f"Pausa anti-sobrecarga entre scrapers ({scraper['delay_after']}s)"
            )
    
    # Resumen del paso
    logger.info("\n" + "=" * 60)
    logger.info(f"ğŸ“Š RESUMEN PASO 1 - SCRAPERS:")
    logger.info(f"   âœ… Exitosos: {successful_scrapers}")
    logger.info(f"   âŒ Fallidos: {failed_scrapers}")
    logger.info(f"   ğŸ“ˆ Tasa Ã©xito: {(successful_scrapers/len(SCRAPERS_CONFIG)*100):.1f}%")
    logger.info("=" * 60)
    
    return successful_scrapers > 0  # Al menos uno debe funcionar

def step_2_combine_data():
    """PASO 2: Combinar datos de todos los scrapers"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ”— PASO 2: COMBINANDO DATOS")
    logger.info("=" * 60)
    
    processor = PROCESSORS_CONFIG[0]  # data-combiner.py
    script_path = SRC_DIR / processor['directory'] / processor['script']
    
    logger.info(f"ğŸ“ {processor['description']}")
    
    success = run_script(
        script_path,
        processor['name'],
        working_dir=SRC_DIR / processor['directory']
    )
    
    if success:
        # Verificar que se generÃ³ combined_data.csv
        combined_path = DATA_PROCESSED_DIR / "combined-data" / "combined_data.csv"
        if check_file_exists(combined_path, "Datos combinados"):
            logger.info("âœ… PASO 2 completado exitosamente")
            return True
    
    logger.error("âŒ PASO 2 fallÃ³")
    return False

def step_3_filter_data():
    """PASO 3: Filtrar y procesar datos de comentarios"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ” PASO 3: FILTRANDO DATOS DE COMENTARIOS")
    logger.info("=" * 60)
    
    processor = PROCESSORS_CONFIG[1]  # filter-advanced.py
    script_path = SRC_DIR / processor['directory'] / processor['script']
    
    logger.info(f"ğŸ“ {processor['description']}")
    
    success = run_script(
        script_path,
        processor['name'], 
        working_dir=SRC_DIR / processor['directory']
    )
    
    if success:
        # Verificar archivos generados
        files_to_check = [
            (DATA_PROCESSED_DIR / "filtered-data" / "filtered_data.csv", "Datos filtrados generales"),
            (DATA_PROCESSED_DIR / "filtered-data" / "filtro1_localizacion.csv", "Filtro localizaciÃ³n"),
            (DATA_PROCESSED_DIR / "filtered-data" / "filtro6_marin.csv", "Filtro MarÃ­n")
        ]
        
        all_files_ok = True
        for file_path, description in files_to_check:
            if not check_file_exists(file_path, description):
                all_files_ok = False
        
        if all_files_ok:
            logger.info("âœ… PASO 3 completado exitosamente")
            return True
    
    logger.error("âŒ PASO 3 fallÃ³")
    return False

def step_4_process_metrics():
    """PASO 4: Procesar mÃ©tricas de visualizaciones"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ“Š PASO 4: PROCESANDO MÃ‰TRICAS")
    logger.info("=" * 60)
    
    # Procesar mÃ©tricas avanzadas (polÃ­ticas)
    logger.info("ğŸ›ï¸ Procesando mÃ©tricas polÃ­ticas...")
    metrics_advanced = PROCESSORS_CONFIG[2]  # filter-advanced-m.py
    script_path_advanced = SRC_DIR / metrics_advanced['directory'] / metrics_advanced['script']
    
    success_advanced = run_script(
        script_path_advanced,
        metrics_advanced['name'],
        working_dir=SRC_DIR / metrics_advanced['directory']
    )
    
    # Procesar mÃ©tricas bÃ¡sicas (visualizaciones)
    logger.info("ğŸ“ˆ Procesando mÃ©tricas de visualizaciones...")
    metrics_basic = PROCESSORS_CONFIG[3]  # filter-basic-m.py  
    script_path_basic = SRC_DIR / metrics_basic['directory'] / metrics_basic['script']
    
    success_basic = run_script(
        script_path_basic,
        metrics_basic['name'],
        working_dir=SRC_DIR / metrics_basic['directory']
    )
    
    # Verificar archivos generados
    if success_advanced and success_basic:
        files_to_check = [
            (DATA_PROCESSED_DIR / "metrics-advanced" / "politicos_totales.csv", "MÃ©tricas polÃ­ticas"),
            (DATA_PROCESSED_DIR / "metrics-data" / "visualizaciones_totales.csv", "MÃ©tricas visualizaciones")
        ]
        
        all_files_ok = True
        for file_path, description in files_to_check:
            if not check_file_exists(file_path, description):
                all_files_ok = False
        
        if all_files_ok:
            logger.info("âœ… PASO 4 completado exitosamente") 
            return True
    
    logger.error("âŒ PASO 4 fallÃ³")
    return False

def step_5_final_verification():
    """PASO 5: VerificaciÃ³n final de todos los CSVs necesarios"""
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ” PASO 5: VERIFICACIÃ“N FINAL")
    logger.info("=" * 60)
    
    # CSVs crÃ­ticos que debe usar la aplicaciÃ³n Streamlit
    critical_files = [
        # MÃ©tricas
        (DATA_PROCESSED_DIR / "metrics-data" / "visualizaciones_totales.csv", "MÃ©tricas de visualizaciones"),
        (DATA_PROCESSED_DIR / "metrics-advanced" / "politicos_totales.csv", "MÃ©tricas polÃ­ticas"),
        
        # Comentarios
        (DATA_PROCESSED_DIR / "filtered-data" / "filtered_data.csv", "Datos filtrados principales"),
        (DATA_PROCESSED_DIR / "filtered-data" / "filtro1_localizacion.csv", "Filtro localizaciÃ³n (O Morrazo/Pontevedra)"),
        (DATA_PROCESSED_DIR / "filtered-data" / "filtro6_marin.csv", "Filtro MarÃ­n especÃ­fico")
    ]
    
    logger.info("ğŸ“‹ Verificando CSVs crÃ­ticos para Streamlit:")
    
    all_critical_ok = True
    for file_path, description in critical_files:
        if check_file_exists(file_path, description):
            # Verificar que el archivo no estÃ© vacÃ­o
            try:
                import pandas as pd
                df = pd.read_csv(file_path)
                logger.info(f"   ğŸ“Š {file_path.name}: {len(df)} filas")
            except Exception as e:
                logger.warning(f"   âš ï¸ Error leyendo {file_path.name}: {e}")
        else:
            all_critical_ok = False
    
    # VerificaciÃ³n de CSVs raw (para referencia del usuario)
    logger.info("\nğŸ“‹ Verificando CSVs raw generados por scrapers:")
    raw_files = [
        (DATA_RAW_DIR / "clean-metrics" / "carriola-marin.csv", "Carriola MarÃ­n"),
        (DATA_RAW_DIR / "clean-csvs" / "cousas-carragal-limpio.csv", "Cousas de Carragal"),
        (DATA_RAW_DIR / "clean-csvs" / "diario-marin-limpio.csv", "Diario MarÃ­n"),
        (DATA_RAW_DIR / "clean-csvs" / "diario-pontevedra-limpio.csv", "Diario Pontevedra"),
        (DATA_RAW_DIR / "clean-csvs" / "psdeg-marin-limpio.csv", "PSOE MarÃ­n"),
        (DATA_RAW_DIR / "clean-csvs" / "voz_galicia-limpio.csv", "Voz de Galicia")
    ]
    
    raw_count = 0
    for file_path, description in raw_files:
        if check_file_exists(file_path, f"Raw {description}"):
            raw_count += 1
    
    logger.info(f"\nğŸ“Š RESUMEN VERIFICACIÃ“N FINAL:")
    logger.info(f"   ğŸ¯ CSVs crÃ­ticos: {'âœ… TODOS OK' if all_critical_ok else 'âŒ ALGUNOS FALLAN'}")
    logger.info(f"   ğŸ“ CSVs raw: {raw_count}/{len(raw_files)} generados")
    
    return all_critical_ok

def print_summary():
    """Resumen final del pipeline"""
    end_time = datetime.now()
    
    summary = f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                    ğŸ‰ PIPELINE COMPLETADO ğŸ‰                â•‘
    â•‘                                                              â•‘
    â•‘  âœ… Scrapers ejecutados                                     â•‘
    â•‘  âœ… Datos combinados                                        â•‘
    â•‘  âœ… Filtros aplicados                                       â•‘
    â•‘  âœ… MÃ©tricas procesadas                                     â•‘
    â•‘  âœ… VerificaciÃ³n final                                      â•‘
    â•‘                                                              â•‘
    â•‘  ğŸš€ La aplicaciÃ³n Streamlit estÃ¡ lista para usar            â•‘
    â•‘                                                              â•‘
    â•‘  Hora finalizaciÃ³n: {end_time.strftime('%H:%M:%S')}          â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """
    print(summary)
    logger.info("Pipeline HorizontAI completado exitosamente")

# ========================
# FUNCIÃ“N PRINCIPAL
# ========================

def main():
    """FunciÃ³n principal del pipeline"""
    start_time = datetime.now()
    
    try:
        # Banner inicial
        print_banner()
        
        # Crear directorios necesarios
        logger.info("ğŸ—ï¸ Preparando estructura de directorios...")
        create_directories()
        
        # PASO 1: Scrapers
        if not step_1_run_scrapers():
            logger.error("ğŸ’¥ FALLO CRÃTICO: NingÃºn scraper funcionÃ³")
            return False
        
        # PASO 2: CombinaciÃ³n 
        if not step_2_combine_data():
            logger.error("ğŸ’¥ FALLO CRÃTICO: No se pudieron combinar datos")
            return False
        
        # PASO 3: Filtrado
        if not step_3_filter_data():
            logger.error("ğŸ’¥ FALLO CRÃTICO: No se pudieron filtrar datos")
            return False
        
        # PASO 4: MÃ©tricas
        if not step_4_process_metrics():
            logger.error("ğŸ’¥ FALLO CRÃTICO: No se pudieron procesar mÃ©tricas")
            return False
        
        # PASO 5: VerificaciÃ³n final
        if not step_5_final_verification():
            logger.error("ğŸ’¥ FALLO CRÃTICO: VerificaciÃ³n final fallÃ³")
            return False
        
        # Resumen exitoso
        print_summary()
        
        # Tiempo total
        total_time = datetime.now() - start_time
        logger.info(f"â±ï¸ Tiempo total de ejecuciÃ³n: {total_time}")
        
        return True
        
    except KeyboardInterrupt:
        logger.error("âš ï¸ Pipeline interrumpido por el usuario")
        return False
    except Exception as e:
        logger.error(f"ğŸ’¥ Error inesperado en pipeline: {e}")
        return False

if __name__ == "__main__":
    # Verificar Python 3.7+
    if sys.version_info < (3, 7):
        print("âŒ Se requiere Python 3.7 o superior")
        sys.exit(1)
    
    # Ejecutar pipeline
    success = main()
    
    # CÃ³digo de salida
    sys.exit(0 if success else 1)